//! # Symmetrix Tensor Folding Engine
//!
//! Cache-aware tensor folding and Morton encoding system for optimal memory layout
//! and CPU cache utilization. This module implements the core memory optimization
//! techniques that enable Symmetrix to achieve superior performance through
//! mathematical data structure organization.
//!
//! ## Core Features
//!
//! - **Morton Encoding**: Z-order curve mapping for spatial locality
//! - **Hilbert Curves**: Alternative space-filling curve implementation
//! - **Cache-Aware Allocation**: L1/L2/L3 cache-optimized memory layout
//! - **Tensor Folding**: Recursive partitioning for cache line alignment
//! - **Dynamic Refolding**: Adaptive memory layout based on access patterns

use num_traits::Zero;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::{Arc, RwLock};
use crossbeam_utils::CachePadded;

/// Errors that can occur in tensor operations
#[derive(Debug, thiserror::Error)]
pub enum TensorError {
    #[error("Invalid tensor dimensions: {0}")]
    InvalidDimensions(String),
    
    #[error("Cache allocation failed: {0}")]
    CacheAllocationError(String),
    
    #[error("Morton encoding failed: {0}")]
    MortonError(String),
    
    #[error("Tensor folding failed: {0}")]
    FoldingError(String),
    
    #[error("Memory alignment error: {0}")]
    AlignmentError(String),
}

pub type TensorResult<T> = Result<T, TensorError>;

/// Cache levels for memory hierarchy optimization
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum CacheLevel {
    L1 = 1,
    L2 = 2,
    L3 = 3,
    Memory = 4,
}

impl CacheLevel {
    /// Get the typical cache size for this level
    pub fn typical_size(self) -> usize {
        match self {
            CacheLevel::L1 => 32 * 1024,      // 32KB
            CacheLevel::L2 => 256 * 1024,     // 256KB
            CacheLevel::L3 => 8 * 1024 * 1024, // 8MB
            CacheLevel::Memory => usize::MAX,   // System memory
        }
    }
    
    /// Get the cache line size for this level
    pub fn line_size(self) -> usize {
        match self {
            CacheLevel::L1 | CacheLevel::L2 => 64, // 64 bytes
            CacheLevel::L3 => 64,                  // 64 bytes
            CacheLevel::Memory => 4096,            // 4KB pages
        }
    }
}

/// Morton encoding utilities for Z-order curve mapping
pub struct MortonEncoding;

impl MortonEncoding {
    /// Encode 2D coordinates to Morton order (Z-order curve)
    pub fn encode_2d(x: u32, y: u32) -> u64 {
        let mut result = 0u64;
        
        for i in 0..32 {
            let x_bit = (x >> i) & 1;
            let y_bit = (y >> i) & 1;
            
            result |= (x_bit as u64) << (2 * i);
            result |= (y_bit as u64) << (2 * i + 1);
        }
        
        result
    }
    
    /// Encode 3D coordinates to Morton order
    pub fn encode_3d(x: u32, y: u32, z: u32) -> u64 {
        let mut result = 0u64;
        
        for i in 0..21 { // 21 bits per dimension for 64-bit result
            let x_bit = (x >> i) & 1;
            let y_bit = (y >> i) & 1;
            let z_bit = (z >> i) & 1;
            
            result |= (x_bit as u64) << (3 * i);
            result |= (y_bit as u64) << (3 * i + 1);
            result |= (z_bit as u64) << (3 * i + 2);
        }
        
        result
    }
    
    /// Decode Morton order to 2D coordinates
    pub fn decode_2d(morton: u64) -> (u32, u32) {
        let mut x = 0u32;
        let mut y = 0u32;
        
        for i in 0..32 {
            let x_bit = (morton >> (2 * i)) & 1;
            let y_bit = (morton >> (2 * i + 1)) & 1;
            
            x |= (x_bit as u32) << i;
            y |= (y_bit as u32) << i;
        }
        
        (x, y)
    }
    
    /// Decode Morton order to 3D coordinates
    pub fn decode_3d(morton: u64) -> (u32, u32, u32) {
        let mut x = 0u32;
        let mut y = 0u32;
        let mut z = 0u32;
        
        for i in 0..21 {
            let x_bit = (morton >> (3 * i)) & 1;
            let y_bit = (morton >> (3 * i + 1)) & 1;
            let z_bit = (morton >> (3 * i + 2)) & 1;
            
            x |= (x_bit as u32) << i;
            y |= (y_bit as u32) << i;
            z |= (z_bit as u32) << i;
        }
        
        (x, y, z)
    }
}

/// Cache-aware tensor block for optimal memory layout
#[derive(Debug, Clone)]
pub struct TensorBlock<T> {
    /// The actual data stored in cache-aligned memory
    pub data: Vec<CachePadded<T>>,
    /// Dimensions of the tensor block
    pub dimensions: Vec<usize>,
    /// Morton encoding index for spatial locality
    pub morton_index: u64,
    /// Target cache level for this block
    pub cache_level: CacheLevel,
    /// Memory alignment (in bytes)
    pub alignment: usize,
}

impl<T: Clone + Zero> TensorBlock<T> {
    /// Create a new tensor block with specified dimensions
    pub fn new(dimensions: Vec<usize>, cache_level: CacheLevel) -> TensorResult<Self> {
        let total_elements: usize = dimensions.iter().product();
        
        if total_elements == 0 {
            return Err(TensorError::InvalidDimensions(
                "Cannot create tensor with zero elements".to_string()
            ));
        }
        
        // Check if block fits in target cache level
        let element_size = std::mem::size_of::<T>();
        let total_size = total_elements * element_size;
        let cache_size = cache_level.typical_size();
        
        if total_size > cache_size {
            tracing::warn!(
                "Tensor block size ({} bytes) exceeds cache level {:?} capacity ({} bytes)",
                total_size, cache_level, cache_size
            );
        }
        
        // Initialize data with cache-padded elements
        let data = vec![CachePadded::new(T::zero()); total_elements];
        
        // Compute Morton index based on dimensions
        let morton_index = match dimensions.len() {
            2 => MortonEncoding::encode_2d(dimensions[0] as u32, dimensions[1] as u32),
            3 => MortonEncoding::encode_3d(
                dimensions[0] as u32, 
                dimensions[1] as u32, 
                dimensions[2] as u32
            ),
            _ => 0, // Fallback for higher dimensions
        };
        
        Ok(Self {
            data,
            dimensions,
            morton_index,
            cache_level,
            alignment: cache_level.line_size(),
        })
    }
    
    /// Get element at multi-dimensional index
    pub fn get(&self, indices: &[usize]) -> TensorResult<&T> {
        let linear_index = self.compute_linear_index(indices)?;
        Ok(&self.data[linear_index])
    }
    
    /// Set element at multi-dimensional index
    pub fn set(&mut self, indices: &[usize], value: T) -> TensorResult<()> {
        let linear_index = self.compute_linear_index(indices)?;
        *self.data[linear_index] = value;
        Ok(())
    }
    
    /// Compute linear index from multi-dimensional indices
    fn compute_linear_index(&self, indices: &[usize]) -> TensorResult<usize> {
        if indices.len() != self.dimensions.len() {
            return Err(TensorError::InvalidDimensions(
                format!("Expected {} indices, got {}", self.dimensions.len(), indices.len())
            ));
        }
        
        let mut linear_index = 0;
        let mut stride = 1;
        
        for (i, &index) in indices.iter().enumerate().rev() {
            if index >= self.dimensions[i] {
                return Err(TensorError::InvalidDimensions(
                    format!("Index {} out of bounds for dimension {} (size {})", 
                           index, i, self.dimensions[i])
                ));
            }
            linear_index += index * stride;
            stride *= self.dimensions[i];
        }
        
        Ok(linear_index)
    }
    
    /// Fold the tensor into smaller cache-friendly blocks
    pub fn fold(&self, target_cache_level: CacheLevel) -> TensorResult<Vec<TensorBlock<T>>> {
        let target_size = target_cache_level.typical_size();
        let element_size = std::mem::size_of::<T>();
        let max_elements = target_size / element_size;
        
        if self.data.len() <= max_elements {
            // Already fits in target cache level
            return Ok(vec![self.clone()]);
        }
        
        // Recursively partition the tensor
        let mut blocks = Vec::new();
        let partition_size = (max_elements as f64).sqrt() as usize;
        
        // For 2D tensors, partition into smaller 2D blocks
        if self.dimensions.len() == 2 {
            let rows = self.dimensions[0];
            let cols = self.dimensions[1];
            
            for row_start in (0..rows).step_by(partition_size) {
                for col_start in (0..cols).step_by(partition_size) {
                    let row_end = (row_start + partition_size).min(rows);
                    let col_end = (col_start + partition_size).min(cols);
                    
                    let block_dims = vec![row_end - row_start, col_end - col_start];
                    let mut block = TensorBlock::new(block_dims, target_cache_level)?;
                    
                    // Copy data to the new block
                    for i in 0..(row_end - row_start) {
                        for j in 0..(col_end - col_start) {
                            let src_indices = vec![row_start + i, col_start + j];
                            let dst_indices = vec![i, j];
                            let value = self.get(&src_indices)?.clone();
                            block.set(&dst_indices, value)?;
                        }
                    }
                    
                    blocks.push(block);
                }
            }
        }
        
        Ok(blocks)
    }
}

/// Main tensor folding engine
#[derive(Debug)]
pub struct TensorFolder {
    /// Cache hierarchy configuration
    cache_config: CacheConfig,
    /// Active tensor blocks organized by cache level
    #[allow(clippy::type_complexity)]
    active_blocks: Arc<RwLock<HashMap<CacheLevel, Vec<Arc<TensorBlock<f64>>>>>>,
    /// Memory usage statistics
    memory_stats: Arc<RwLock<MemoryStats>>,
}

/// Cache configuration parameters
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheConfig {
    pub l1_size: usize,
    pub l2_size: usize,
    pub l3_size: usize,
    pub line_size: usize,
    pub associativity: usize,
}

impl Default for CacheConfig {
    fn default() -> Self {
        Self {
            l1_size: 32 * 1024,      // 32KB
            l2_size: 256 * 1024,     // 256KB
            l3_size: 8 * 1024 * 1024, // 8MB
            line_size: 64,           // 64 bytes
            associativity: 8,        // 8-way associative
        }
    }
}

/// Memory usage statistics
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
pub struct MemoryStats {
    pub total_allocated: usize,
    pub l1_usage: usize,
    pub l2_usage: usize,
    pub l3_usage: usize,
    pub cache_hits: u64,
    pub cache_misses: u64,
}

impl TensorFolder {
    /// Create a new tensor folder with the given cache configuration
    pub fn new(cache_config: CacheConfig) -> Self {
        Self {
            cache_config,
            active_blocks: Arc::new(RwLock::new(HashMap::new())),
            memory_stats: Arc::new(RwLock::new(MemoryStats::default())),
        }
    }
    
    /// Allocate a new tensor with optimal cache placement
    pub fn allocate_tensor(&self, dimensions: Vec<usize>) -> TensorResult<Arc<TensorBlock<f64>>> {
        let total_elements: usize = dimensions.iter().product();
        let element_size = std::mem::size_of::<f64>();
        let total_size = total_elements * element_size;
        
        // Determine optimal cache level
        let cache_level = if total_size <= self.cache_config.l1_size {
            CacheLevel::L1
        } else if total_size <= self.cache_config.l2_size {
            CacheLevel::L2
        } else if total_size <= self.cache_config.l3_size {
            CacheLevel::L3
        } else {
            CacheLevel::Memory
        };
        
        let tensor_block = Arc::new(TensorBlock::new(dimensions, cache_level)?);
        
        // Register the block
        let mut active_blocks = self.active_blocks.write().unwrap();
        active_blocks.entry(cache_level)
            .or_default()
            .push(tensor_block.clone());
        
        // Update memory statistics
        let mut stats = self.memory_stats.write().unwrap();
        stats.total_allocated += total_size;
        match cache_level {
            CacheLevel::L1 => stats.l1_usage += total_size,
            CacheLevel::L2 => stats.l2_usage += total_size,
            CacheLevel::L3 => stats.l3_usage += total_size,
            CacheLevel::Memory => {},
        }
        
        tracing::debug!(
            "Allocated tensor with {} elements ({} bytes) in cache level {:?}",
            total_elements, total_size, cache_level
        );
        
        Ok(tensor_block)
    }
    
    /// Get current memory usage statistics
    pub fn get_memory_stats(&self) -> MemoryStats {
        self.memory_stats.read().unwrap().clone()
    }
    
    /// Optimize memory layout by refolding tensors
    pub fn optimize_layout(&self) -> TensorResult<()> {
        let active_blocks = self.active_blocks.read().unwrap();
        
        for (cache_level, blocks) in active_blocks.iter() {
            tracing::info!(
                "Optimizing {} blocks in cache level {:?}",
                blocks.len(), cache_level
            );
            
            // TODO: Implement adaptive refolding based on access patterns
            // This would analyze which tensor blocks are accessed together
            // and reorganize them for better cache locality
        }
        
        Ok(())
    }
}

/// Cache-aware tensor type alias
pub type CacheAwareTensor<T> = TensorBlock<T>;

/// Initialize the tensor folding engine
pub fn initialize_tensor_engine(cache_size: usize) -> Result<TensorFolder, Box<dyn std::error::Error>> {
    let cache_config = CacheConfig {
        l3_size: cache_size,
        ..Default::default()
    };

    let folder = TensorFolder::new(cache_config);
    tracing::info!("Tensor folding engine initialized with {}MB cache", cache_size / (1024 * 1024));
    Ok(folder)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_morton_encoding_2d() {
        let (x, y) = (5, 3);
        let morton = MortonEncoding::encode_2d(x, y);
        let (decoded_x, decoded_y) = MortonEncoding::decode_2d(morton);
        
        assert_eq!(x, decoded_x);
        assert_eq!(y, decoded_y);
    }

    #[test]
    fn test_morton_encoding_3d() {
        let (x, y, z) = (7, 4, 2);
        let morton = MortonEncoding::encode_3d(x, y, z);
        let (decoded_x, decoded_y, decoded_z) = MortonEncoding::decode_3d(morton);
        
        assert_eq!(x, decoded_x);
        assert_eq!(y, decoded_y);
        assert_eq!(z, decoded_z);
    }

    #[test]
    fn test_tensor_block_creation() {
        let dimensions = vec![4, 4];
        let block = TensorBlock::<f64>::new(dimensions.clone(), CacheLevel::L1);
        
        assert!(block.is_ok());
        let block = block.unwrap();
        assert_eq!(block.dimensions, dimensions);
        assert_eq!(block.cache_level, CacheLevel::L1);
    }

    #[test]
    fn test_tensor_block_access() {
        let dimensions = vec![3, 3];
        let mut block = TensorBlock::<f64>::new(dimensions, CacheLevel::L1).unwrap();
        
        let indices = vec![1, 2];
        let value = 42.0;
        
        block.set(&indices, value).unwrap();
        let retrieved = *block.get(&indices).unwrap();
        
        assert_eq!(retrieved, value);
    }

    #[test]
    fn test_tensor_folder() {
        let config = CacheConfig::default();
        let folder = TensorFolder::new(config);
        
        let dimensions = vec![10, 10];
        let tensor = folder.allocate_tensor(dimensions);
        
        assert!(tensor.is_ok());
        
        let stats = folder.get_memory_stats();
        assert!(stats.total_allocated > 0);
    }
}
